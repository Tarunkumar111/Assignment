{"cells":[{"metadata":{"id":"o9fciGc7s5Vu"},"cell_type":"markdown","source":"<h1> 1. Business Problem </h1>"},{"metadata":{"id":"LRzmxjKxs5Vw"},"cell_type":"markdown","source":"<h2> 1.1 Description </h2>"},{"metadata":{"id":"1nlaIYe9s5Vx"},"cell_type":"markdown","source":"<p>Many donors, experts, and microfinance institutions (MFI) have become convinced that using mobile financial services (MFS) is more convenient and efficient, and less costly, than the traditional high-touch model for delivering microfinance services. MFS becomes especially useful when targeting the unbanked poor living in remote areas. The implementation of MFS, though, has been uneven with both significant challenges and successes.\nToday, microfinance is widely accepted as a poverty-reduction tool, representing $70 billion in outstanding loans and a global outreach of 200 million clients. </p>\n<p>\nOne of our Client in Telecom collaborates with an MFI to provide micro-credit on mobile balances to be paid back in 5 days. The Consumer is believed to be delinquent if he deviates from the path of paying back the loaned amount within 5 days. \n</p>\n<br>\n"},{"metadata":{"id":"wdWP5SdFs5Vy"},"cell_type":"markdown","source":"__ Problem Statement __\n\nClassify the given transaction based on evidence from customer information."},{"metadata":{"id":"jlNRUR4Ws5V5"},"cell_type":"markdown","source":"<h2>1.2 Real world/Business Objectives and Constraints </h2>"},{"metadata":{"id":"Hv6fd7txs5V7"},"cell_type":"markdown","source":"* We would want a probability for each loan transaction, whether the customer will be paying back the loaned amount within five days of insurance of loan so that we can choose any threshold of choice.\n* No strict latency concerns.\n* Interpretability is partially important."},{"metadata":{"id":"VIam5Aaks5V9"},"cell_type":"markdown","source":"<h1>2. Machine Learning Probelm </h1>"},{"metadata":{"id":"rty1PZv3s5V_"},"cell_type":"markdown","source":"<h2> 2.1 Data Overview </h2>"},{"metadata":{"id":"-gu8pAt3s5WB"},"cell_type":"markdown","source":"<p> \n- Data will be in a file sample_data_intw.csv <br>\n- Csv file contains 37 columns. <br>\n- Number of rows in sample_data_intw.csv = 209593\n</p>"},{"metadata":{"id":"9qPVfeEjs5WF"},"cell_type":"markdown","source":"<h2> 2.2 Mapping the real world problem to an ML problem </h2>"},{"metadata":{"id":"JfBn0LYPs5WI"},"cell_type":"markdown","source":"<h3> 2.2.1 Type of Machine Leaning Problem </h3>"},{"metadata":{"id":"QEqiUD_Ps5WJ"},"cell_type":"markdown","source":"<p> It is a binary classification problem, for given transaction detail we need to predict the customer will be paying back the loaned amount within five days or not. </p>"},{"metadata":{"id":"keZOL1las5WL"},"cell_type":"markdown","source":"<h3> 2.2.2 Performance Metric </h3>"},{"metadata":{},"cell_type":"markdown","source":"Metric(s):\n* log-loss\n* Binary Confusion Matrix"},{"metadata":{"id":"FmDMBWJjs5WO"},"cell_type":"markdown","source":"<h2> 2.3 Train and Test Construction </h2>"},{"metadata":{"id":"l7PcvKQss5WQ"},"cell_type":"markdown","source":"<p>  </p>\n<p> We build train and test by randomly splitting in the ratio of 70:30 or 80:20 whatever we choose as we have sufficient points to work with. </p>"},{"metadata":{"id":"cW_MVIlps5WQ"},"cell_type":"markdown","source":"<h1>3. Exploratory Data Analysis </h1>"},{"metadata":{"id":"sNzZdmBJs5WS","outputId":"0e1df4ed-4a74-4b0e-e84e-1b3862bbf55d","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport time\nimport warnings\nimport numpy as np\nimport seaborn as sns\nfrom collections import Counter, defaultdict\nfrom nltk.corpus import stopwords\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import normalize\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.manifold import TSNE\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, normalized_mutual_info_score\nfrom sklearn.metrics.classification import accuracy_score, log_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\nfrom scipy.sparse import hstack\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n\nimport math\nfrom sklearn.ensemble import RandomForestClassifier\nwarnings.filterwarnings(\"ignore\")\n\nfrom mlxtend.classifier import StackingClassifier\n\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"id":"__T8jddGs5Wc"},"cell_type":"markdown","source":"<h2> 3.1 Reading data and basic stats </h2>"},{"metadata":{"id":"ifM_s9rvs5Wd","outputId":"2e17a7bc-9a5b-4c43-d35b-081cc9f92528","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/loan-with5days/sample_data_intw.csv\")\n\nprint(\"Number of data points:\",df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"id":"34zXGW8gs5Wj","outputId":"ab7d570a-9eeb-477a-b7cb-663ff6fd04fa","trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"mx4DFwMns5Wp","outputId":"1141e0bb-2750-489e-8b8c-2ba680f7416c","trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"ZulqVzTDs5Wx"},"cell_type":"markdown","source":"<h2> 3.2 Distribution of data points among output classes</h2>"},{"metadata":{},"cell_type":"markdown","source":"The number of data points belonging to class one(1) and class zero(0). "},{"metadata":{"id":"YHp64yNjs5Wx","outputId":"361ddf04-d545-45f9-dbe2-8bebd695e8da","trusted":true},"cell_type":"code","source":"df.groupby(\"label\")['Unnamed: 0'].count().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"id":"-usI2K2bs5W4","outputId":"ff0a6a8b-65ad-487a-d5ec-df3c223ba620","trusted":true},"cell_type":"code","source":"print('~> Total number of data points for training and testing:\\n   {}'.format(len(df)))","execution_count":null,"outputs":[]},{"metadata":{"id":"YiPia6Pjs5W_","outputId":"3cde4cec-4314-4c14-e807-b35e969bf9e8","trusted":true},"cell_type":"code","source":"print('~> Data point belongs to class zero (label = 0):\\n   {}%'.format(100 - round(df['label'].mean()*100, 2)))\nprint('\\n~> Data point belongs to class one (label = 1):\\n   {}%'.format(round(df['label'].mean()*100, 2)))","execution_count":null,"outputs":[]},{"metadata":{"id":"G-CwGaMms5XS"},"cell_type":"markdown","source":"<h3>3.2.1 Checking for Duplicates </h3>"},{"metadata":{"id":"YCiDBHm5s5XT","outputId":"d8011926-4086-4c9a-9fcf-59663a584ec4","trusted":true},"cell_type":"code","source":"#checking duplicates\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"h_WdYxlYs5Xj"},"cell_type":"markdown","source":"<h3> 3.2.3 Checking for NULL values </h3>"},{"metadata":{"id":"r0x1gR2fs5Xk","outputId":"721aef48-e628-40c6-d567-25466f4283e1","trusted":true},"cell_type":"code","source":"#Checking whether there are any rows with null values\nnan_rows = df[df.isnull().any(1)]\nprint (nan_rows)","execution_count":null,"outputs":[]},{"metadata":{"id":"CCYmufv6s5Xo"},"cell_type":"markdown","source":"- There are no null value in given data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df.iloc[:,3:-2]\ny=df.iloc[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX = sc_X.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>4. Machine Learning Models </h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the dataset into train and test\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0,test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of data points in train data :\",X_train.shape)\nprint(\"Number of data points in test data :\",X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"-\"*10,\"Distribution of output variable in train data\",\"-\"*10)\ntrain_distr=Counter(y_train)\ntrain_len=len(y_train)\nprint(\"Class 0: \",int(train_distr[0])/train_len,\"Class 1: \",int(train_distr[1])/train_len)\nprint(\"-\"*10,\"Distribution of output variable in test data\",\"-\"*10)\ntest_distr=Counter(y_test)\ntest_len=len(y_test)\nprint(\"Class 0: \",int(test_distr[1])/test_len,\"Class 1: \",int(test_distr[1])/test_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function plots the confusion matrices given y_i, y_i_hat.\ndef plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n    \n    A =(((C.T)/(C.sum(axis=1))).T)\n    #divid each element of the confusion matrix with the sum of elements in that column\n    \n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.T = [[1, 3],\n    #        [2, 4]]\n    # C.sum(axis = 1)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =1) = [[3, 7]]\n    # ((C.T)/(C.sum(axis=1))) = [[1/3, 3/7]\n    #                           [2/3, 4/7]]\n\n    # ((C.T)/(C.sum(axis=1))).T = [[1/3, 2/3]\n    #                           [3/7, 4/7]]\n    # sum of row elements = 1\n    \n    B =(C/C.sum(axis=0))\n    #divid each element of the confusion matrix with the sum of elements in that row\n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.sum(axis = 0)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =0) = [[4, 6]]\n    # (C/C.sum(axis=0)) = [[1/4, 2/6],\n    #                      [3/4, 4/6]] \n    plt.figure(figsize=(20,4))\n    \n    labels = [1,2]\n    # representing A in heatmap format\n    cmap=sns.light_palette(\"blue\")\n    plt.subplot(1, 3, 1)\n    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Confusion matrix\")\n    \n    plt.subplot(1, 3, 2)\n    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Precision matrix\")\n    \n    plt.subplot(1, 3, 3)\n    # representing B in heatmap format\n    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Recall matrix\")\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> 4.1 Building a random model (Finding worst-case log-loss) </h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_y = np.zeros((test_len,2))\nfor i in range(test_len):\n    rand_probs = np.random.rand(1,2)\n    predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\nprint(\"Log loss on Test Data using Random Model\",log_loss(y_test, predicted_y, eps=1e-15))\n\npredicted_y =np.argmax(predicted_y, axis=1)\nplot_confusion_matrix(y_test, predicted_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> 4.2 Logistic Regression with hyperparameter tuning </h2>"},{"metadata":{},"cell_type":"markdown","source":"<h3>4.2.1 Without class balancing</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = [10 ** x for x in range(-5, 3)] # hyperparam for SGD classifier.\nlog_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(X_train, y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_train, y_train)\n    predict_y = sig_clf.predict_proba(X_test)\n    log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, log_error_array,c='g')\nfor i, txt in enumerate(np.round(log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(X_train, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_train, y_train)\n\npredict_y = sig_clf.predict_proba(X_train)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\npredicted_y =np.argmax(predict_y,axis=1)\nprint(\"Total number of data points :\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> 4.3 Logistic Regression with hyperparameter tuning </h2>"},{"metadata":{},"cell_type":"markdown","source":"<h3> 4.3.1 With class balancing</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = [10 ** x for x in range(-5, 2)] # hyperparameter for SGD classifier.\nlog_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(class_weight='balanced',alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(X_train, y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_train, y_train)\n    predict_y = sig_clf.predict_proba(X_test)\n    log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, log_error_array,c='g')\nfor i, txt in enumerate(np.round(log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(log_error_array)\nclf = SGDClassifier(class_weight='balanced',alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(X_train, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_train, y_train)\n\npredict_y = sig_clf.predict_proba(X_train)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\npredicted_y =np.argmax(predict_y,axis=1)\nprint(\"Total number of data points :\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"1.Quora.ipynb","provenance":[],"version":"0.3.2"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}